{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Role Prompting\n",
    "\n",
    "Role-play prompting is a common prompt engineering technique designed to enhance the reasoning capabilities of large language models (LLMs) in zero-shot settings. This approach involves assigning a specific role to the LLM, such as a math teacher or an expert in a particular field, before presenting it with a task or question. The technique sometimes employs a two-stage framework: first, constructing task-specific role-play prompts that define the LLM's persona, and second, using these prompts to elicit responses for reasoning queries. \n",
    "\n",
    "By immersing the LLM in a carefully chosen role, role-play prompting aims to activate relevant knowledge and reasoning patterns, leading to more accurate and contextually appropriate responses. This method has shown promising results across various reasoning tasks, often outperforming standard zero-shot approaches and serving as an effective implicit trigger for chain-of-thought reasoning. Role-play prompting represents a simple yet powerful way to leverage the LLM's ability to adopt different personas, potentially unlocking improved performance in complex reasoning tasks without the need for few-shot examples or fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Better Zero-Shot Reasoning with Role-Play Prompting\n",
    "\n",
    "This paper introduces a novel approach called \"role-play prompting\" to enhance the reasoning capabilities of large language models (LLMs) in zero-shot settings. The method involves a two-stage framework: first, constructing task-specific role-play prompts, and second, using these prompts to elicit responses for reasoning queries. The authors evaluate their approach on 12 diverse reasoning benchmarks using ChatGPT and other open-source LLMs. Results show that role-play prompting consistently outperforms standard zero-shot approaches and often surpasses Zero-Shot-CoT on most datasets. \n",
    "\n",
    "The study suggests that role-play prompting serves as an effective implicit Chain-of-Thought trigger, leading to improved reasoning outcomes. The authors also explore the impact of prompt design and role selection on performance. Overall, this work demonstrates the potential of role-play prompting as a simple yet effective technique for enhancing LLMs' reasoning abilities and opens up new avenues for research at the intersection of role-playing and reasoning in LLMs.\n",
    "\n",
    "> [Better Zero-Shot Reasoning with Role-Play Prompting](https://arxiv.org/abs/2308.07702) by Zao, S. (2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard Answer: Product description: A shoe that fits any foot size.\n",
      "Product names: FlexiFit Shoe, UniversalStride, AnySize Slip-On\n",
      "\n",
      "Roleplay Answer: Product description: A shoe that fits any foot size\n",
      "Product names: OmniShoeX, Not a regular shoe, The Universal Shoe Company\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "def role_play_prompt(role_setting, prompt):\n",
    "    \"\"\"\n",
    "    Function to perform role-play prompting using ChatGPT.\n",
    "    \"\"\"\n",
    "    conversation = [\n",
    "        {\"role\": \"system\", \"content\": role_setting},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=conversation,\n",
    "        temperature=1,\n",
    "        max_tokens=512\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content\n",
    "\n",
    "response_format = \"\"\"Return the results as a comma separated list, in this format:\n",
    "Product description: [product description]\n",
    "Product names: [list of 3 product names]\"\"\"\n",
    "\n",
    "role_examples = \"\"\"Product description: Brainstorm a list of product names for a refridgerator that dispenses beer\n",
    "Product names: BeerFridgeX, Not a beer fridge, The Beer Fridge Company\n",
    "\n",
    "Product description: A watch that can tell accurate time in space\n",
    "Product names: WatchX, Not a watch, The Watch Company\n",
    "\n",
    "Product description: A home milkshake maker\n",
    "Product names: MilkShakeX, Not a milkshake maker, The Milkshake Maker Company\n",
    "\n",
    "Product description: An electric car that can drive underwater\n",
    "Product names: AquaCarX, Not a submarine, The Amphibious Auto Company\n",
    "\n",
    "Product description: A smartphone that can project holograms\n",
    "Product names: HoloPhoneX, Not a projector, The Holographic Handset Company\"\"\"\n",
    "\n",
    "role_setting = f\"\"\"You are Elon Musk, and you are brainstorming names for new products.\n",
    "{response_format}\n",
    "## Examples\n",
    "{role_examples}\"\"\"\n",
    "\n",
    "prompt = \"\"\"Brainstorm a list of product names for a shoe that fits any foot size.\"\"\"\n",
    "\n",
    "standard_response = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": response_format},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "    temperature=1,\n",
    "    max_tokens=512\n",
    ")\n",
    "\n",
    "standard_answer = standard_response.choices[0].message.content\n",
    "print(\"Standard Answer:\", standard_answer)\n",
    "print()\n",
    "\n",
    "answer = role_play_prompt(role_setting, prompt)\n",
    "print(\"Roleplay Answer:\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation of Roleplay Names:\n",
      "**OmniShoeX**\n",
      "Explanation: This name fits well with Elon Musk's style. The use of \"Omni\" suggests a comprehensive or universal approach, and the addition of \"X\" at the end follows the pattern seen in products like SpaceX and Model X. It has a futuristic and bold feel.\n",
      "Elon Musk likelihood: 90%\n",
      "\n",
      "**Not a regular shoe**\n",
      "Explanation: This name is playful and directly follows the format \"Not a [product]\" which is reminiscent of \"Not a Flamethrower.\" It has a humorous and straightforward style that aligns with Musk's branding.\n",
      "Elon Musk likelihood: 95%\n",
      "\n",
      "**The Universal Shoe Company**\n",
      "Explanation: This name is straightforward and follows the template \"The [product] company.\" While it is a bit more conventional and less playful than the other two names, it still fits within the realm of Musk's naming conventions.\n",
      "Elon Musk likelihood: 80%\n",
      "\n",
      "Evaluation of Standard Names:\n",
      "**FlexiFit Shoe**\n",
      "\n",
      "Explanation: This name has a practical and descriptive quality to it, which is not entirely in line with Elon Musk's more bold and futuristic naming conventions. It lacks the playful or innovative twist that Musk often incorporates into his product names.\n",
      "\n",
      "Elon Musk likelihood: 20%\n",
      "\n",
      "**UniversalStride**\n",
      "\n",
      "Explanation: This name leans more towards the futuristic and ambitious tone that Elon Musk often uses, suggesting a product that is universally applicable and advanced. However, it still misses a bit of the playful or unexpected element Musk might add.\n",
      "\n",
      "Elon Musk likelihood: 40%\n",
      "\n",
      "**AnySize Slip-On**\n",
      "\n",
      "Explanation: This name is practical and descriptive but lacks the bold, futuristic, or playful twist characteristic of Musk's naming style. It sounds more like a conventional product name rather than something innovative or unexpected.\n",
      "\n",
      "Elon Musk likelihood: 15%\n"
     ]
    }
   ],
   "source": [
    "def evaluate_elon_musk_names(names):\n",
    "    evaluation_prompt = f\"\"\"As an AI language model with knowledge of Elon Musk's style and naming conventions, evaluate the following product names and determine if they sound like they were created by Elon Musk. Consider his tendency for futuristic, bold, and sometimes playful names.\n",
    "\n",
    "Product names: {names}\n",
    "\n",
    "Provide your evaluation as a percentage (0-100%) indicating how likely these names are to have been created by Elon Musk. Also, provide a brief explanation for your rating.\n",
    "\n",
    "Name templates that are deemed to be 100% in the style of Elon Musk are:\n",
    "- [Product]X\n",
    "- The [product] company\n",
    "- Not a [product]\n",
    "\n",
    "Response format:\n",
    "Explanation: [brief explanation]\n",
    "Elon Musk likelihood: [percentage]\n",
    "\"\"\"\n",
    "\n",
    "    evaluation_response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an AI expert in evaluating product names and branding styles.\"},\n",
    "            {\"role\": \"user\", \"content\": evaluation_prompt}\n",
    "        ],\n",
    "        temperature=0.5,\n",
    "        max_tokens=1000\n",
    "    )\n",
    "\n",
    "    return evaluation_response.choices[0].message.content\n",
    "\n",
    "# Extract product names from the roleplay answer\n",
    "roleplay_names = answer.split(\"Product names: \")[1].strip()\n",
    "\n",
    "# Evaluate the names\n",
    "evaluation_result = evaluate_elon_musk_names(roleplay_names)\n",
    "print(\"Evaluation of Roleplay Names:\")\n",
    "print(evaluation_result)\n",
    "\n",
    "# For comparison, let's evaluate the standard answer as well\n",
    "standard_names = standard_answer.split(\"Product names: \")[1].strip()\n",
    "standard_evaluation_result = evaluate_elon_musk_names(standard_names)\n",
    "print(\"\\nEvaluation of Standard Names:\")\n",
    "print(standard_evaluation_result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
